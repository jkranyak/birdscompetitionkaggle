{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 70203,
          "databundleVersionId": 8068726,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30684,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebooka3489e63aa",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkranyak/birdscompetitionkaggle/blob/main/notebooka3489e63aa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'birdclef-2024:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F70203%2F8068726%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240411%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240411T234502Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D46e0e22533c4fcd389c01d0e4a59680f0195c48981d89b38fbcc56169788c4e2ec0cd598173d4b060798187fc098d9ae8be44f928e19332fd7a04dd9291b7cd5812e6ccb7a55cde8fb478dd218fbb7838d1668800383c690cadf71fdb848aef8fc877fe66aa901c88f53b367b5159910d89ce41d7d6e5efaa398ab4edb3a3698eb791cf66685b93ce0b06ccbe72381b605d5a1cd8666e4a5b0c3deed470d2e40bcebf7459e89a08001c6c206049a51e26d8448f234521076fa17a96a87092b56e9f57708917dce91638fcc740d304c9522406f9230270c574117522f32a1760132f49b58bdb9ccae232fd02fe940370249bd14d548dc193e0be81f80f0badebf'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkagW9VijPL_",
        "outputId": "7fb7dc27-6f44-4e77-f81b-a695ce8e84c9"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading birdclef-2024, 23390009647 bytes compressed\n",
            "[=============================                     ] 13794672640 bytes downloaded"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-04-11T23:32:49.552887Z",
          "iopub.execute_input": "2024-04-11T23:32:49.553549Z",
          "iopub.status.idle": "2024-04-11T23:32:55.164783Z",
          "shell.execute_reply.started": "2024-04-11T23:32:49.553516Z",
          "shell.execute_reply": "2024-04-11T23:32:55.163536Z"
        },
        "trusted": true,
        "id": "lu13AVocjPMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa soundfile tensorflow scikit-learn numpy pandas matplotlib seaborn tqdm\n",
        "!pip install audiomentations\n",
        "!pip install tqdm\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-11T23:35:59.34574Z",
          "iopub.execute_input": "2024-04-11T23:35:59.34623Z",
          "iopub.status.idle": "2024-04-11T23:36:42.312614Z",
          "shell.execute_reply.started": "2024-04-11T23:35:59.346192Z",
          "shell.execute_reply": "2024-04-11T23:36:42.311225Z"
        },
        "trusted": true,
        "id": "rUe5amTcjPMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Flatten, LSTM, TimeDistributed, Dense, Dropout, concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "from keras.utils import Sequence\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import soundfile as sf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import multiprocessing as mp\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import joblib\n",
        "from multiprocessing import Pool\n",
        "from scipy import signal\n",
        "from scipy.io import wavfile\n",
        "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-11T23:36:42.333826Z",
          "iopub.execute_input": "2024-04-11T23:36:42.334282Z",
          "iopub.status.idle": "2024-04-11T23:36:42.350386Z",
          "shell.execute_reply.started": "2024-04-11T23:36:42.334239Z",
          "shell.execute_reply": "2024-04-11T23:36:42.349414Z"
        },
        "trusted": true,
        "id": "K6oHmAOhjPMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "G5n082Z2yIID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k# Define output directories\n",
        "output_directories = ['processed_data', 'models', 'submissions', 'visualizations']\n",
        "base_working_dir = '/kaggle/working/'\n",
        "\n",
        "# Creating subdirectories\n",
        "for directory in output_directories:\n",
        "    path = os.path.join(base_working_dir, directory)\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    print(f\"Created directory: {path}\")\n",
        "\n",
        "# Define base directories for input data\n",
        "base_input_dir = \"/kaggle/input/birdclef-2024\"\n",
        "\n",
        "# Paths for specific data types\n",
        "train_audio_base_path = os.path.join(base_input_dir, \"train_audio\")\n",
        "unlabeled_soundscapes_path = os.path.join(base_input_dir, \"unlabeled_soundscapes\")\n",
        "train_metadata_csv_path = os.path.join(base_input_dir, \"train_metadata.csv\")\n",
        "taxonomy_csv_path = os.path.join(base_input_dir, \"eBird_Taxonomy_v2021.csv\")\n",
        "sample_submission_csv_path = os.path.join(base_input_dir, \"sample_submission.csv\")\n",
        "test_soundscapes_path = os.path.join(base_input_dir, \"test_soundscapes\")\n",
        "\n",
        "# Function to list all audio files considering subdirectories\n",
        "def list_all_audio_files(base_path):\n",
        "    all_files = []\n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        for file in files:\n",
        "            if file.endswith('.ogg'):\n",
        "                all_files.append(os.path.join(root, file))\n",
        "    return all_files\n",
        "\n",
        "# Example usage\n",
        "all_train_audio_files = list_all_audio_files(train_audio_base_path)\n",
        "print(f\"Total audio files found: {len(all_train_audio_files)}\")\n"
      ],
      "metadata": {
        "id": "zRcUvdsrj_dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Audio Processing and Spectrogram Generation\n",
        "\n",
        "In this section, we focus on converting raw audio data into a more visually interpretable form called a spectrogram, which is crucial for our Convolutional Recurrent Neural Network (CRNN) model. Spectrograms provide a 2D representation of the sound where the x-axis represents time, the y-axis represents frequency, and the intensity of colors shows the amplitude of a particular frequency at a particular time.\n",
        "\n",
        "#### Step-by-step Process:\n",
        "\n",
        "1. **Audio Loading**: Each audio file is loaded into the memory, which captures the sound's waveform as a time-series data.\n",
        "2. **Spectrogram Conversion**: The raw audio is then converted into a spectrogram using the Mel scale, which is more aligned with human auditory perception.\n",
        "3. **Visualization and Saving**: The generated spectrogram is visualized and saved as a PNG file. This image serves as the input to our neural network.\n",
        "\n",
        "The following visualization is an example of a spectrogram generated from our dataset. This particular audio file represents the call of a bird species recorded in the Western Ghats. By analyzing such images, our model learns to identify patterns specific to different bird calls.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9zRpCGT-1F0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of generating a spectrogram\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load an example audio file\n",
        "y, sr = librosa.load(librosa.ex('trumpet'))\n",
        "\n",
        "# Create a Mel-spectrogram\n",
        "S = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "S_DB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "# Plot the Mel-spectrogram\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(S_DB, sr=sr, x_axis='time', y_axis='mel')\n",
        "plt.colorbar(format='%+2.0f dB')\n",
        "plt.title('Mel-frequency spectrogram')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R-5L2tXE3BYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M5Spqb3p0ijM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "# Load metadata\n",
        "metadata_path = '/kaggle/input/birdclef-2024/train_metadata.csv'\n",
        "metadata = pd.read_csv(metadata_path)\n",
        "filtered_metadata = metadata[metadata['latitude'].between(8, 21) & metadata['longitude'].between(72, 78)]\n",
        "\n",
        "# Define the base directory for train audio\n",
        "base_audio_dir = '/kaggle/input/birdclef-2024/train_audio'\n",
        "output_base_dir = '/kaggle/working/processed_data'\n",
        "\n",
        "# Check and construct full audio paths, verify existence\n",
        "filtered_metadata.loc[:, 'audio_path'] = filtered_metadata.apply(lambda row: create_audio_path(row, base_audio_dir), axis=1)\n",
        "filtered_metadata.loc[:, 'exists'] = filtered_metadata['audio_path'].apply(os.path.exists)\n",
        "\n",
        "# Filter out non-existing files\n",
        "valid_files_metadata = filtered_metadata[filtered_metadata['exists']]\n",
        "\n",
        "def process_and_save_spectrogram(row):\n",
        "    \"\"\"Process each audio file, generate spectrogram, and save in designated folder.\"\"\"\n",
        "    audio_path = row['audio_path']\n",
        "    primary_label = row['primary_label'] if pd.notna(row['primary_label']) else 'unknown'\n",
        "    output_dir = os.path.join(output_base_dir, primary_label)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        y, sr = librosa.load(audio_path, sr=None)\n",
        "        S = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "        S_DB = librosa.power_to_db(S, ref=np.max)\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        librosa.display.specshow(S_DB, sr=sr, x_axis='time', y_axis='mel')\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "        plt.title(f'Mel-frequency spectrogram of {primary_label}')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        save_path = os.path.join(output_dir, f\"{os.path.splitext(os.path.basename(audio_path))[0]}_spectrogram.png\")\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "        return f\"Saved: {save_path}\"\n",
        "    except Exception as e:\n",
        "        return f\"Failed to process {audio_path}: {str(e)}\"\n",
        "\n",
        "# Process and save spectrograms\n",
        "results = [process_and_save_spectrogram(row) for _, row in tqdm(valid_files_metadata.iterrows(), total=valid_files_metadata.shape[0])]\n",
        "\n",
        "# Optionally print the results\n",
        "for result in results[:10]:  # Just print the first 10 results\n",
        "    print(result)"
      ],
      "metadata": {
        "id": "zF69tSyVueRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Design: Convolutional Recurrent Neural Network (CRNN)\n",
        "\n",
        "The CRNN model combines the spatial feature extraction capabilities of Convolutional Neural Networks (CNNs) with the sequence modeling prowess of Recurrent Neural Networks (RNNs), making it ideal for audio classification tasks like bird call identification.\n",
        "\n",
        "#### Architecture Overview:\n",
        "\n",
        "- **Convolutional Layers**: These layers extract a hierarchy of high-level features from the spectrogram images.\n",
        "- **Recurrent Layer**: An LSTM layer processes the sequence of features extracted by the CNN to capture temporal dependencies.\n",
        "- **Output Layer**: Outputs the probability distribution across all bird species, predicting the likelihood of each species being present in the audio clip.\n",
        "\n",
        "#### Why CRNN?\n",
        "\n",
        "- **Spatial Feature Extraction**: The CNN layers effectively identify spatial features in spectrograms, such as shapes and textures associated with various bird sounds.\n",
        "- **Temporal Sequence Modeling**: The LSTM layer captures the temporal dynamics of bird calls, crucial for distinguishing between similar-sounding bird species.\n",
        "- **Efficiency**: CRNNs can handle large input sizes and are effective in environments with limited labeled data, which is typical in bird sound classification.\n",
        "\n",
        "This hybrid model structure leverages the strengths of both CNNs and RNNs, providing a robust framework for our bird species classification task.\n",
        "\n",
        "The next steps will involve training this model on our processed dataset, evaluating its performance, and refining the approach based on the observed results.\n"
      ],
      "metadata": {
        "id": "sXKOv5B21ilY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_crnn_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        TimeDistributed(Conv2D(32, kernel_size=(3, 3), activation='relu'), input_shape=input_shape),\n",
        "        TimeDistributed(BatchNormalization()),\n",
        "        TimeDistributed(MaxPooling2D(pool_size=(2, 2))),\n",
        "        TimeDistributed(Conv2D(64, kernel_size=(3, 3), activation='relu')),\n",
        "        TimeDistributed(BatchNormalization()),\n",
        "        TimeDistributed(MaxPooling2D(pool_size=(2, 2))),\n",
        "        TimeDistributed(Flatten()),\n",
        "        LSTM(64, return_sequences=False),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define model parameters\n",
        "input_shape = (None, 128, 128, 1)  # Example input shape (time_steps, height, width, channels)\n",
        "num_classes = 182  # Number of bird species\n",
        "\n",
        "# Create the CRNN model\n",
        "model = create_crnn_model(input_shape, num_classes)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "8V_OXoNwsDYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AiisGx4z4bjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Define the directory where spectrogram images are saved\n",
        "processed_data_dir = '/kaggle/working/processed_data'\n",
        "\n",
        "# Function to load spectrogram images from a directory\n",
        "def load_spectrogram_images(directory):\n",
        "    spectrogram_images = []\n",
        "    labels = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith('_spectrogram.png'):\n",
        "                # Load the image\n",
        "                image_path = os.path.join(root, file)\n",
        "                image = Image.open(image_path)\n",
        "                # Convert to numpy array and normalize\n",
        "                spectrogram = np.array(image) / 255.0  # Normalize to [0, 1]\n",
        "                # Append to the list of spectrogram images\n",
        "                spectrogram_images.append(spectrogram)\n",
        "                # Extract the label from the directory name\n",
        "                label = os.path.basename(root)\n",
        "                labels.append(label)\n",
        "    return np.array(spectrogram_images), np.array(labels)\n",
        "\n",
        "# Load spectrogram images and corresponding labels\n",
        "features, labels = load_spectrogram_images(processed_data_dir)\n",
        "\n",
        "# Print the shape of features and labels\n",
        "print(\"Shape of features:\", features.shape)\n",
        "print(\"Number of labels:\", len(labels))\n"
      ],
      "metadata": {
        "id": "9wTSLm_E4bxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the Code\n",
        "\n",
        "#### Model Setup and Compilation\n",
        "- **Importing Libraries:**\n",
        "  - The code begins by importing necessary modules from TensorFlow and Matplotlib.\n",
        "- **Hyperparameters:**\n",
        "  - Learning rate, epochs, and batch size are defined.\n",
        "- **Model Compilation:**\n",
        "  - The model is compiled using the Adam optimizer with a specified learning rate and categorical cross-entropy loss function. Accuracy is chosen as the evaluation metric.\n",
        "\n",
        "#### Callbacks Setup\n",
        "- **Model Checkpoint:**\n",
        "  - This callback saves the model weights to 'best_model.h5' file when validation accuracy improves.\n",
        "- **Early Stopping:**\n",
        "  - This callback monitors validation loss and stops training if there's no improvement for a specified number of epochs (patience).\n",
        "\n",
        "#### Model Training\n",
        "- **Training the Model:**\n",
        "  - The `fit()` function is called to train the model on the training dataset (`train_dataset`) for the specified number of epochs. Validation data (`val_dataset`) is provided for monitoring the model's performance during training. Callbacks are passed to this function for checkpointing and early stopping.\n",
        "\n",
        "#### Visualization\n",
        "- **Training History Plotting:**\n",
        "  - Matplotlib is used to visualize the training and validation accuracy over epochs. This helps in assessing the model's performance and identifying overfitting or underfitting issues.\n"
      ],
      "metadata": {
        "id": "zQGDyc8E38FW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Callbacks Setup\n",
        "\n",
        "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# # Model Checkpoint callback\n",
        "# checkpoint_filepath = '/kaggle/working/models/best_model.h5'\n",
        "# model_checkpoint_callback = ModelCheckpoint(\n",
        "#     filepath=checkpoint_filepath,\n",
        "#     save_weights_only=True,\n",
        "#     monitor='val_accuracy',\n",
        "#     mode='max',\n",
        "#     save_best_only=True)\n",
        "\n",
        "# # Early Stopping callback\n",
        "# early_stopping_callback = EarlyStopping(\n",
        "#     monitor='val_loss',\n",
        "#     patience=5,\n",
        "#     restore_best_weights=True\n",
        "# )\n",
        "\n",
        "# # Model Training\n",
        "\n",
        "# history = model.fit(\n",
        "#     train_dataset,\n",
        "#     epochs=50,\n",
        "#     validation_data=val_dataset,\n",
        "#     callbacks=[model_checkpoint_callback, early_stopping_callback]\n",
        "# )\n",
        "\n",
        "# # Visualization\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Plot training & validation accuracy values\n",
        "# plt.plot(history.history['accuracy'])\n",
        "# plt.plot(history.history['val_accuracy'])\n",
        "# plt.title('Model accuracy')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "# plt.show()\n",
        "\n",
        "# # Plot training & validation loss values\n",
        "# plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['val_loss'])\n",
        "# plt.title('Model loss')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "4QhHiGIwmvd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZoDq7b8E3682"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wcsKsGMx3SIW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}