{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30684,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkranyak/birdscompetitionkaggle/blob/kaggle/Kaggle_pre_loaded_model_usage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'birdclef-2024:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F70203%2F8068726%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240411%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240411T234502Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D46e0e22533c4fcd389c01d0e4a59680f0195c48981d89b38fbcc56169788c4e2ec0cd598173d4b060798187fc098d9ae8be44f928e19332fd7a04dd9291b7cd5812e6ccb7a55cde8fb478dd218fbb7838d1668800383c690cadf71fdb848aef8fc877fe66aa901c88f53b367b5159910d89ce41d7d6e5efaa398ab4edb3a3698eb791cf66685b93ce0b06ccbe72381b605d5a1cd8666e4a5b0c3deed470d2e40bcebf7459e89a08001c6c206049a51e26d8448f234521076fa17a96a87092b56e9f57708917dce91638fcc740d304c9522406f9230270c574117522f32a1760132f49b58bdb9ccae232fd02fe940370249bd14d548dc193e0be81f80f0badebf'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-12T19:38:03.559098Z",
          "iopub.execute_input": "2024-04-12T19:38:03.559510Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6oKtFB020m8",
        "outputId": "e5b3f560-6cef-45be-f8a3-b8e841577e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading birdclef-2024, 23390009647 bytes compressed\n",
            "[==============                                    ] 6795264000 bytes downloaded"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "trusted": true,
        "id": "Cn7UNDgt20m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model from TensorFlow Hub\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.model_download(\"google/bird-vocalization-classifier/tensorFlow2/bird-vocalization-classifier\")\n",
        "\n",
        "print(\"Path to model files:\", path)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "zisrbLAE20nA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the file path is correct and the file exists\n",
        "metadata_path = '/kaggle/input/birdclef-2024/train_metadata.csv'\n",
        "metadata = pd.read_csv(metadata_path)\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "display(metadata.head())\n",
        "\n",
        "# Display basic information about the dataframe\n",
        "print(metadata.info())"
      ],
      "metadata": {
        "id": "0my5oHzy71jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the metadata\n",
        "metadata_path = '/kaggle/input/birdclef-2024/train_metadata.csv'\n",
        "metadata = pd.read_csv(metadata_path)\n",
        "\n",
        "# Apply the expanded geographic filter for the Western Ghats and surrounding migratory areas\n",
        "western_ghats_birds = metadata[\n",
        "    (metadata['latitude'].between(8, 20)) &\n",
        "    (metadata['longitude'].between(72, 80))\n",
        "]\n",
        "\n",
        "# Handle missing values if necessary (drop or impute)\n",
        "western_ghats_birds = western_ghats_birds.dropna(subset=['latitude', 'longitude'])\n",
        "\n",
        "# Print the new DataFrame to verify the filter and see the first few rows\n",
        "print(western_ghats_birds.head())\n",
        "print(western_ghats_birds.info())\n",
        "\n",
        "# Optional: Save this DataFrame to a new CSV for easier access or further analysis\n",
        "output_path = '/kaggle/working/western_ghats_birds.csv'\n",
        "western_ghats_birds.to_csv(output_path, index=False)\n",
        "print(f\"DataFrame saved to {output_path}\")\n"
      ],
      "metadata": {
        "id": "GhUph-5Q86BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is already loaded\n",
        "df_path = '/kaggle/working/western_ghats_birds.csv'\n",
        "df = pd.read_csv(df_path)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df = df.drop(['author', 'license', 'rating', 'url', 'secondary_labels', 'type'], axis=1)\n",
        "\n",
        "# Optionally, save the trimmed DataFrame for further use\n",
        "trimmed_df_path = '/kaggle/working/western_ghats_birds_trimmed.csv'\n",
        "df.to_csv(trimmed_df_path, index=False)\n",
        "\n",
        "print(\"Trimmed DataFrame saved:\", trimmed_df_path)\n",
        "print(df.head())  # Show the first few entries of the trimmed DataFrame\n"
      ],
      "metadata": {
        "id": "8T_2inZnFb51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "\n",
        "base_audio_dir = '/kaggle/input/birdclef-2024/train_audio'\n",
        "\n",
        "def load_audio_data(row):\n",
        "    file_path = os.path.join(base_audio_dir, row['filename'])\n",
        "    audio, sr = librosa.load(file_path)\n",
        "    return audio, sr\n",
        "\n",
        "# Example of loading audio for the first few filtered entries\n",
        "for index, row in filtered_metadata.head().iterrows():\n",
        "    audio, sr = load_audio_data(row)\n",
        "    print(f\"Loaded audio from {row['filename']} with sample rate {sr}\")\n"
      ],
      "metadata": {
        "id": "_2Te9fFY80vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "# Load the DataFrame\n",
        "df_path = '/kaggle/working/western_ghats_birds.csv'\n",
        "western_ghats_birds = pd.read_csv(df_path)\n",
        "\n",
        "# Base directory where original audio files are stored\n",
        "base_audio_dir = '/kaggle/input/birdclef-2024/train_audio'\n",
        "\n",
        "# Directory to store the copied audio files\n",
        "target_dir = '/kaggle/working/processed_embeddings/western_ghats_bird_calls'\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Initialize a counter for copied files\n",
        "files_copied = 0\n",
        "\n",
        "# Iterate through the DataFrame and copy files\n",
        "for idx, row in western_ghats_birds.iterrows():\n",
        "    src_path = os.path.join(base_audio_dir, row['filename'])\n",
        "    if os.path.exists(src_path):\n",
        "        # Ensure the target subdirectory exists\n",
        "        subfolder_path = os.path.join(target_dir, os.path.dirname(row['filename']))\n",
        "        os.makedirs(subfolder_path, exist_ok=True)\n",
        "\n",
        "        # Define the destination path\n",
        "        dest_path = os.path.join(target_dir, row['filename'])\n",
        "\n",
        "        # Copy the file, replacing it if it already exists\n",
        "        shutil.copy2(src_path, dest_path)  # Use copy2 for better handling of metadata and overwriting\n",
        "        files_copied += 1  # Increment the counter\n",
        "        print(f\"Copied and replaced if existing: {src_path} to {dest_path}\")\n",
        "    else:\n",
        "        print(f\"File not found: {src_path}\")\n",
        "\n",
        "# Print the total number of files copied\n",
        "print(f\"All specified audio files have been processed. Total files copied or replaced: {files_copied}\")\n"
      ],
      "metadata": {
        "id": "P1NEJzclAomP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the pre-trained model from TensorFlow Hub\n",
        "model_url = '/root/.cache/kagglehub/models/google/bird-vocalization-classifier/tensorFlow2/bird-vocalization-classifier/8'\n",
        "model = hub.load(model_url)\n",
        "\n",
        "def load_and_preprocess_audio(file_path, sr=32000, duration=5):\n",
        "    \"\"\" Load and preprocess audio to have consistent duration and sample rate \"\"\"\n",
        "    audio, _ = librosa.load(file_path, sr=sr, duration=duration)\n",
        "    if len(audio) < sr * duration:\n",
        "        audio = np.pad(audio, (0, sr * duration - len(audio)), 'constant')\n",
        "    return audio\n",
        "\n",
        "def extract_features(audio):\n",
        "    \"\"\" Extract embedding features from the pre-trained model \"\"\"\n",
        "    audio_tensor = tf.convert_to_tensor([audio], dtype=tf.float32)\n",
        "    outputs = model.signatures['serving_default'](audio_tensor)\n",
        "    embeddings = outputs['embedding'].numpy()\n",
        "    return embeddings.flatten()\n",
        "\n",
        "# Load metadata and filtered audio file paths\n",
        "df_path = '/kaggle/working/western_ghats_birds.csv'\n",
        "df = pd.read_csv(df_path)\n",
        "\n",
        "# Directory with the relevant audio files\n",
        "audio_dir = '/kaggle/working/processed_embeddings/western_ghats_bird_calls'\n",
        "\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "# Process each file in the DataFrame\n",
        "for _, row in df.iterrows():\n",
        "    audio_path = os.path.join(audio_dir, row['filename'])\n",
        "    if os.path.exists(audio_path):\n",
        "        audio_data = load_and_preprocess_audio(audio_path)\n",
        "        features.append(extract_features(audio_data))\n",
        "        labels.append(row['primary_label'])\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a RandomForest classifier\n",
        "classifier = RandomForestClassifier(n_estimators=100)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the classifier\n",
        "predictions = classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "print(\"Model trained and evaluated.\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "9E2JeGvpAwbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Directory where the segmented audio files will be stored\n",
        "segmented_audio_dir = '/kaggle/working/western_ghats_audio_segments'\n",
        "os.makedirs(segmented_audio_dir, exist_ok=True)  # Ensure the directory exists\n",
        "\n",
        "# Function to load audio, segment it into 5-second clips, and save the segments\n",
        "def load_and_segment_audio(file_path, output_dir, filename, sr=32000, segment_length=5):\n",
        "    audio, _ = librosa.load(file_path, sr=sr)\n",
        "    segments = []\n",
        "    segment_count = 0\n",
        "    for start in range(0, len(audio), sr * segment_length):\n",
        "        end = start + sr * segment_length\n",
        "        if end <= len(audio):\n",
        "            segment_path = os.path.join(output_dir, f\"{filename}_segment_{segment_count}.wav\")\n",
        "            librosa.output.write_wav(segment_path, audio[start:end], sr)\n",
        "            segments.append(segment_path)\n",
        "            segment_count += 1\n",
        "    return segments\n",
        "\n",
        "# Function to extract features using the pre-trained model\n",
        "def extract_features(model, audio_path):\n",
        "    audio_data, _ = librosa.load(audio_path, sr=32000)\n",
        "    audio_tensor = tf.constant(audio_data, dtype=tf.float32)\n",
        "    audio_tensor = tf.reshape(audio_tensor, [1, -1])  # Reshape for batch dimension\n",
        "    outputs = model.signatures['serving_default'](inputs=audio_tensor)\n",
        "    features = outputs['embedding']\n",
        "    return features.numpy()\n",
        "\n",
        "# Load the pre-trained model\n",
        "model_url = '/root/.cache/kagglehub/models/google/bird-vocalization-classifier/tensorFlow2/bird-vocalization-classifier/8'\n",
        "feature_extractor = hub.load(model_url)\n",
        "\n",
        "# Load the metadata for Western Ghats bird calls\n",
        "df_path = '/kaggle/working/western_ghats_birds.csv'\n",
        "df = pd.read_csv(df_path)\n",
        "\n",
        "# Prepare data arrays\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "# Process each file in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    file_path = os.path.join(base_audio_dir, row['filename'])\n",
        "    if os.path.exists(file_path):\n",
        "        audio_segments = load_and_segment_audio(file_path, segmented_audio_dir, os.path.splitext(row['filename'])[0])\n",
        "        for segment_path in audio_segments:\n",
        "            extracted_features = extract_features(feature_extractor, segment_path)\n",
        "            features.append(extracted_features.flatten())  # Flatten the features\n",
        "            labels.append(row['primary_label'])\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a RandomForest classifier\n",
        "classifier = RandomForestClassifier(n_estimators=100)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the classifier\n",
        "predictions = classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "h8UqQ5GGTEOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F3pZPxx8tcJ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}